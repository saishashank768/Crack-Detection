# -*- coding: utf-8 -*-
"""Untitled44.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mOcnVYxtowo2-auRVbCFz_dLmQky59NS
"""

import tensorflow as tf
import tensorflow_hub as hub

# Load the MobileNetV3 Large model from TensorFlow Hub
model = tf.keras.Sequential([
    hub.KerasLayer("https://tfhub.dev/google/imagenet/mobilenet_v3_large_100_224/feature_vector/5",
                   input_shape=(224, 224, 3),
                   trainable=False)
])

import numpy as np
import os
import cv2
from sklearn.model_selection import train_test_split
from tensorflow.keras.applications import MobileNetV3Large
from tensorflow.keras.layers import Dense, Flatten
from tensorflow.keras.models import Model
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.optimizers import Adam
from sklearn.metrics import confusion_matrix

# Preprocess input images for MobileNetV3Large
preprocess_input = tf.keras.applications.mobilenet_v3.preprocess_input

import zipfile
import os

# Provide the path to your zip file
zip_file_path = "/content/reduced cracks dataset (3).zip"

# Provide the directory where you want to extract the contents
extracted_dir_path = "/content/sample_data"

# Create the target directory if it doesn't exist
os.makedirs(extracted_dir_path, exist_ok=True)

# Create a ZipFile object
with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:
    # Extract all the contents into the specified directory
    zip_ref.extractall(extracted_dir_path)

# Print a message indicating the extraction is complete
print("Extraction complete.")

# Set the path to your dataset folder
dataset_path = "/content/sample_data/reduced cracks dataset/TRAIN DATA IMAGES"

# Set the input image size for VGG19
image_size = (224, 224)
input_shape = (224,224,3)

##tf.keras.applications.inception_resnet_v2.preprocess_input

# Load the VGG19 model
base_model = MobileNetV3Large(include_top=False, weights='imagenet', input_shape=(224, 224, 3))

# Freeze the pre-trained layers
for layer in base_model.layers:
    layer.trainable = False

# Add custom fully connected layers on top of VGG19
x = base_model.output
x = Flatten()(x)
x = Dense(512, activation='relu')(x)
x = Dense(256, activation='relu')(x)

# Calculate the number of classes based on the unique labels in the dataset
num_classes = len(os.listdir(dataset_path))

predictions = Dense(num_classes, activation='softmax')(x)

# Create the final model
model = Model(inputs=base_model.input, outputs=predictions)

# Compile the model
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Load and preprocess the images
images = []
labels = []
label_mapping = {}  # To map label indices to their respective labels
for i, label in enumerate(os.listdir(dataset_path)):
    label_mapping[i] = label
    label_path = os.path.join(dataset_path, label)
    for image_file in os.listdir(label_path):
        image_path = os.path.join(label_path, image_file)
        image = cv2.imread(image_path)
        image = cv2.resize(image, image_size)
        image = preprocess_input(image)
        images.append(image)
        labels.append(i)  # Use label index instead of one-hot encoding

# Convert images and labels to numpy arrays
images = np.array(images)
labels = np.array(labels)

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(images, labels, test_size=0.2, random_state=42)

# Perform data augmentation on the training set
train_datagen = ImageDataGenerator(rotation_range=20, width_shift_range=0.1, height_shift_range=0.1,
                                   shear_range=0.1, zoom_range=0.2, horizontal_flip=True, fill_mode='nearest')
train_datagen.fit(X_train)

# Convert labels to one-hot encoding
y_train_onehot = np.eye(num_classes, dtype=np.int32)[y_train]
y_test_onehot = np.eye(num_classes, dtype=np.int32)[y_test]

# Train the model
#history = model.fit(train_datagen.flow(X_train, y_train_onehot, batch_size=32), epochs=10)

# Train the model
history = model.fit(train_datagen.flow(X_train, y_train_onehot, batch_size=32), epochs=10,
                    validation_data=(X_test, y_test_onehot))

import matplotlib.pyplot as plt
#plt.figure(figsize=(8, 8))
#plt.subplot(1, 2, 1)
#plt.plot(range(EPOCHS), acc, label='Training Accuracy')
#plt.plot(range(EPOCHS), val_acc, label='Validation Accuracy')
#plt.legend(loc='lower right')
plt.title('MobileNetV3 - Training and Validation Loss')
plt.xlabel("Epochs")
plt.ylabel("Accuracy")
plt.plot(history.history['loss'], label='train loss')
plt.plot(history.history['val_loss'], label='val loss')
plt.legend()
plt.grid()
plt.show()
plt.savefig('LossVal_loss')
# plt.subplot(1, 2, 2)
# plt.plot(range(EPOCHS), loss, label='Training Loss')
#plt.plot(range(EPOCHS), val_loss, label='Validation Loss')
# plt.legend(loc='upper right')
plt.title('MobileNetV3 -Training and Validation Accuracy')
plt.xlabel("Epochs")
plt.ylabel("Loss")
# plt.show()
# accuracies
plt.plot(history.history['accuracy'], label='train_acc')
plt.plot(history.history['val_accuracy'], label='val_acc')
plt.legend()
plt.grid()
plt.show()
plt.savefig('AccVal_acc')

# Make predictions on the testing set
y_pred = model.predict(X_test)
y_pred = np.argmax(y_pred, axis=1)

# Compute the confusion matrix
confusion_mtx = confusion_matrix(y_test, y_pred)
print(confusion_mtx)

# Compute the true labels
true_labels = np.argmax(y_test_onehot, axis=1)

# Make predictions on the test dataset
y_pred = model.predict(X_test)
predicted_labels = np.argmax(y_pred, axis=1)

import numpy as np
import matplotlib.pyplot as plt
import itertools
from sklearn.metrics import confusion_matrix, classification_report

# Assuming you have a trained model called 'model' and the test dataset is stored in 'X_test' and 'y_test_onehot'
# model = ...
# X_test = ...
# y_test_onehot = ...

# Compute the true labels
true_labels = np.argmax(y_test_onehot, axis=1)
true_labels = [str(label) for label in true_labels]

# Make predictions on the test dataset
y_pred = model.predict(X_test)
predicted_labels = np.argmax(y_pred, axis=1)
predicted_labels = [str(label) for label in predicted_labels]

# Define the class labels
class_labels = ["Diagonal", "Horizontal", "Structural", "Vertical"]
#unique_classes = np.unique(np.concatenate((true_labels, predicted_labels)))
#class_labels = [str(cls) for cls in unique_classes]
#class_labels = np.unique(np.concatenate((true_labels, predicted_labels)))
#class_labels = list(X_train.class_indices.keys())


# Compute the confusion matrix
confusion_mtx = confusion_matrix(true_labels, predicted_labels)

# Function to plot the confusion matrix
def plot_confusion_matrix(cm, class_labels, normalize=False, title='Inception ResNet V2 - Confusion Matrix', cmap=plt.cm.Oranges):
    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]

    plt.figure(figsize=(8, 8))
    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title)
    plt.colorbar()
    tick_marks = np.arange(len(class_labels))
    plt.xticks(tick_marks, class_labels, rotation=90)
    plt.yticks(tick_marks, class_labels)

    fmt = '.2f' if normalize else 'd'
    thresh = cm.max() / 2.
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j, i, format(cm[i, j], fmt),
                 horizontalalignment="center",
                 color="white" if cm[i, j] > thresh else "black")

    plt.ylabel('True Label')
    plt.xlabel('Predicted Label')
    plt.tight_layout()

# Plot the confusion matrix
plot_confusion_matrix(confusion_mtx, class_labels, normalize=False)

# Show the plot
plt.show()

# Print the classification report
classification_rep = classification_report(true_labels, predicted_labels, target_names=class_labels)
print(classification_rep)

# Select a random index from the test set
index = np.random.choice(len(X_test), 1)[0]
#index = np.random.choice(16, 1)[0]

# Display the actual image and label
fig, ax = plt.subplots(figsize=(2, 2))
actual_image = np.clip(X_test[index], 0, 1)  # Clip pixel values to range [0, 1]
plt.imshow(actual_image, interpolation='nearest')
ax.set_title('Actual label: %s' % class_labels[y_test[index].argmax()])
ax.axis('off')
plt.show()

# Display the predicted image and label
fig, ax = plt.subplots(figsize=(2, 2))
plt.imshow(X_test[index])
ax.set_title('Predicted label: %s' % class_labels[y_pred[index].argmax()])
ax.axis('off')
plt.show()

"""Resnet_101"""

tf.keras.models.save_model(model,'my_model1.hdf5')

import numpy as np
import os
import cv2
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, classification_report
from keras.applications.resnet import ResNet101, preprocess_input
from keras.layers import Dense, Flatten
from keras.models import Model
from keras.preprocessing.image import ImageDataGenerator

# Set the path to your dataset folder
dataset_path = "/content/sample_data/reduced cracks dataset/TRAIN DATA IMAGES"

# Set the input image size for VGG19
image_size = (224, 224)

# Load the VGG19 model
base_model = ResNet101(weights='imagenet', include_top=False, input_shape=image_size + (3,))

# Freeze the pre-trained layers
for layer in base_model.layers:
    layer.trainable = False

# Add custom fully connected layers on top of VGG19
x = base_model.output
x = Flatten()(x)
x = Dense(512, activation='relu')(x)
x = Dense(256, activation='relu')(x)

# Calculate the number of classes based on the unique labels in the dataset
num_classes = len(os.listdir(dataset_path))

predictions = Dense(num_classes, activation='softmax')(x)

# Create the final model
model = Model(inputs=base_model.input, outputs=predictions)

# Compile the model
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Load and preprocess the images
images = []
labels = []
label_mapping = {}  # To map label indices to their respective labels
for i, label in enumerate(os.listdir(dataset_path)):
    label_mapping[i] = label
    label_path = os.path.join(dataset_path, label)
    for image_file in os.listdir(label_path):
        image_path = os.path.join(label_path, image_file)
        image = cv2.imread(image_path)
        image = cv2.resize(image, image_size)
        image = preprocess_input(image)
        images.append(image)
        labels.append(i)  # Use label index instead of one-hot encoding

# Convert images and labels to numpy arrays
images = np.array(images)
labels = np.array(labels)

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(images, labels, test_size=0.2, random_state=42)

# Perform data augmentation on the training set
train_datagen = ImageDataGenerator(rotation_range=20, width_shift_range=0.1, height_shift_range=0.1,
                                   shear_range=0.1, zoom_range=0.2, horizontal_flip=True, fill_mode='nearest')
train_datagen.fit(X_train)

# Convert labels to one-hot encoding
y_train_onehot = np.eye(num_classes, dtype=np.int32)[y_train]
y_test_onehot = np.eye(num_classes, dtype=np.int32)[y_test]

# Train the model
#history = model.fit(train_datagen.flow(X_train, y_train_onehot, batch_size=32), epochs=10)

# Train the model
history = model.fit(train_datagen.flow(X_train, y_train_onehot, batch_size=32), epochs=10,
                    validation_data=(X_test, y_test_onehot))

import matplotlib.pyplot as plt

plt.title('ResNet101 - Training and Validation Loss')
plt.xlabel("Epochs")
plt.ylabel("Accuracy")
plt.plot(history.history['loss'], label='train loss')
plt.plot(history.history['val_loss'], label='val loss')
plt.legend()
plt.grid()
plt.show()
plt.savefig('LossVal_loss')
# plt.subplot(1, 2, 2)
# plt.plot(range(EPOCHS), loss, label='Training Loss')
#plt.plot(range(EPOCHS), val_loss, label='Validation Loss')
# plt.legend(loc='upper right')
plt.title('ResNet101 -Training and Validation Accuracy')
plt.xlabel("Epochs")
plt.ylabel("Loss")
# plt.show()
# accuracies
plt.plot(history.history['accuracy'], label='train_acc')
plt.plot(history.history['val_accuracy'], label='val_acc')
plt.legend()
plt.grid()
plt.show()
plt.savefig('AccVal_acc')

# Make predictions on the testing set
y_pred = model.predict(X_test)
y_pred = np.argmax(y_pred, axis=1)

# Compute the confusion matrix
confusion_mtx = confusion_matrix(y_test, y_pred)
print(confusion_mtx)

# Compute the true labels
true_labels = np.argmax(y_test_onehot, axis=1)

# Make predictions on the test dataset
y_pred = model.predict(X_test)
predicted_labels = np.argmax(y_pred, axis=1)

import numpy as np
import matplotlib.pyplot as plt
import itertools
from sklearn.metrics import confusion_matrix, classification_report

# Assuming you have a trained model called 'model' and the test dataset is stored in 'X_test' and 'y_test_onehot'
# model = ...
# X_test = ...
# y_test_onehot = ...

# Compute the true labels
true_labels = np.argmax(y_test_onehot, axis=1)
true_labels = [str(label) for label in true_labels]

# Make predictions on the test dataset
y_pred = model.predict(X_test)
predicted_labels = np.argmax(y_pred, axis=1)
predicted_labels = [str(label) for label in predicted_labels]

# Define the class labels
class_labels = ["Diagonal", "Horizontal", "Structural", "Vertical"]
#unique_classes = np.unique(np.concatenate((true_labels, predicted_labels)))
#class_labels = [str(cls) for cls in unique_classes]
#class_labels = np.unique(np.concatenate((true_labels, predicted_labels)))
#class_labels = list(X_train.class_indices.keys())


# Compute the confusion matrix
confusion_mtx = confusion_matrix(true_labels, predicted_labels)

# Function to plot the confusion matrix
def plot_confusion_matrix(cm, class_labels, normalize=False, title='ResNet101 - Confusion Matrix', cmap=plt.cm.Oranges):
    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]

    plt.figure(figsize=(4, 4))
    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title)
    plt.colorbar()
    tick_marks = np.arange(len(class_labels))
    plt.xticks(tick_marks, class_labels, rotation=90)
    plt.yticks(tick_marks, class_labels)

    fmt = '.2f' if normalize else 'd'
    thresh = cm.max() / 2.
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j, i, format(cm[i, j], fmt),
                 horizontalalignment="center",
                 color="white" if cm[i, j] > thresh else "black")

    plt.ylabel('True Label')
    plt.xlabel('Predicted Label')
    plt.tight_layout()

# Plot the confusion matrix
plot_confusion_matrix(confusion_mtx, class_labels, normalize=False)

# Show the plot
plt.show()

# Print the classification report
classification_rep = classification_report(true_labels, predicted_labels, target_names=class_labels)
print(classification_rep)

# Select a random index from the test set
index = np.random.choice(len(X_test), 1)[0]
#index = np.random.choice(16, 1)[0]

# Display the actual image and label
fig, ax = plt.subplots(figsize=(2, 2))
actual_image = np.clip(X_test[index], 0, 1)  # Clip pixel values to range [0, 1]
plt.imshow(actual_image, interpolation='nearest')
ax.set_title('Actual label: %s' % class_labels[y_test[index].argmax()])
ax.axis('off')
plt.show()

# Display the predicted image and label
fig, ax = plt.subplots(figsize=(2, 2))
plt.imshow(X_test[index])
ax.set_title('Predicted label: %s' % class_labels[y_pred[index].argmax()])
ax.axis('off')
plt.show()

tf.keras.models.save_model(model,'my_model2.hdf5')

"""Resnet_152"""

import numpy as np
import os
import cv2
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, classification_report
from keras.applications.resnet import ResNet152, preprocess_input
from keras.layers import Dense, Flatten
from keras.models import Model
from keras.preprocessing.image import ImageDataGenerator

# Set the path to your dataset folder
dataset_path = "/content/sample_data/reduced cracks dataset/TRAIN DATA IMAGES"

# Set the input image size for VGG19
image_size = (224, 224)

# Load the VGG19 model
base_model = ResNet152(weights='imagenet', include_top=False, input_shape=image_size + (3,))

# Freeze the pre-trained layers
for layer in base_model.layers:
    layer.trainable = False

# Add custom fully connected layers on top of VGG19
x = base_model.output
x = Flatten()(x)
x = Dense(512, activation='relu')(x)
x = Dense(256, activation='relu')(x)

# Calculate the number of classes based on the unique labels in the dataset
num_classes = len(os.listdir(dataset_path))

predictions = Dense(num_classes, activation='softmax')(x)

# Create the final model
model = Model(inputs=base_model.input, outputs=predictions)

# Compile the model
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Convert images and labels to numpy arrays
images = np.array(images)
labels = np.array(labels)

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(images, labels, test_size=0.2, random_state=42)

# Perform data augmentation on the training set
train_datagen = ImageDataGenerator(rotation_range=20, width_shift_range=0.1, height_shift_range=0.1,
                                   shear_range=0.1, zoom_range=0.2, horizontal_flip=True, fill_mode='nearest')
train_datagen.fit(X_train)

# Convert labels to one-hot encoding
y_train_onehot = np.eye(num_classes, dtype=np.int32)[y_train]
y_test_onehot = np.eye(num_classes, dtype=np.int32)[y_test]

# Train the model
#history = model.fit(train_datagen.flow(X_train, y_train_onehot, batch_size=32), epochs=10)

# Train the model
history = model.fit(train_datagen.flow(X_train, y_train_onehot, batch_size=32), epochs=10,
                    validation_data=(X_test, y_test_onehot))

import matplotlib.pyplot as plt


plt.title('ResNet152 - Training and Validation Loss')
plt.xlabel("Epochs")
plt.ylabel("Accuracy")
plt.plot(history.history['loss'], label='train loss')
plt.plot(history.history['val_loss'], label='val loss')
plt.legend()
plt.grid()
plt.show()
plt.savefig('LossVal_loss')
# plt.subplot(1, 2, 2)
# plt.plot(range(EPOCHS), loss, label='Training Loss')
#plt.plot(range(EPOCHS), val_loss, label='Validation Loss')
# plt.legend(loc='upper right')
plt.title('ResNet152 -Training and Validation Accuracy')
plt.xlabel("Epochs")
plt.ylabel("Loss")
# plt.show()
# accuracies
plt.plot(history.history['accuracy'], label='train_acc')
plt.plot(history.history['val_accuracy'], label='val_acc')
plt.legend()
plt.grid()
plt.show()
plt.savefig('AccVal_acc')

# Make predictions on the testing set
y_pred = model.predict(X_test)
y_pred = np.argmax(y_pred, axis=1)

# Compute the confusion matrix
confusion_mtx = confusion_matrix(y_test, y_pred)
print(confusion_mtx)

# Compute the true labels
true_labels = np.argmax(y_test_onehot, axis=1)

# Make predictions on the test dataset
y_pred = model.predict(X_test)
predicted_labels = np.argmax(y_pred, axis=1)

import numpy as np
import matplotlib.pyplot as plt
import itertools
from sklearn.metrics import confusion_matrix, classification_report

# Assuming you have a trained model called 'model' and the test dataset is stored in 'X_test' and 'y_test_onehot'
# model = ...
# X_test = ...
# y_test_onehot = ...

# Compute the true labels
true_labels = np.argmax(y_test_onehot, axis=1)
true_labels = [str(label) for label in true_labels]

# Make predictions on the test dataset
y_pred = model.predict(X_test)
predicted_labels = np.argmax(y_pred, axis=1)
predicted_labels = [str(label) for label in predicted_labels]

# Define the class labels
class_labels = ["Diagonal", "Horizontal", "Structural", "Vertical"]
#unique_classes = np.unique(np.concatenate((true_labels, predicted_labels)))
#class_labels = [str(cls) for cls in unique_classes]
#class_labels = np.unique(np.concatenate((true_labels, predicted_labels)))
#class_labels = list(X_train.class_indices.keys())


# Compute the confusion matrix
confusion_mtx = confusion_matrix(true_labels, predicted_labels)

# Function to plot the confusion matrix
def plot_confusion_matrix(cm, class_labels, normalize=False, title='ResNet152 - Confusion Matrix', cmap=plt.cm.Oranges):
    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]

    plt.figure(figsize=(8, 8))
    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title)
    plt.colorbar()
    tick_marks = np.arange(len(class_labels))
    plt.xticks(tick_marks, class_labels, rotation=90)
    plt.yticks(tick_marks, class_labels)

    fmt = '.2f' if normalize else 'd'
    thresh = cm.max() / 2.
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j, i, format(cm[i, j], fmt),
                 horizontalalignment="center",
                 color="white" if cm[i, j] > thresh else "black")

    plt.ylabel('True Label')
    plt.xlabel('Predicted Label')
    plt.tight_layout()

# Plot the confusion matrix
plot_confusion_matrix(confusion_mtx, class_labels, normalize=False)

# Show the plot
plt.show()

# Print the classification report
classification_rep = classification_report(true_labels, predicted_labels, target_names=class_labels)
print(classification_rep)

# Select a random index from the test set
index = np.random.choice(len(X_test), 1)[0]
#index = np.random.choice(16, 1)[0]

# Display the actual image and label
fig, ax = plt.subplots(figsize=(2, 2))
actual_image = np.clip(X_test[index], 0, 1)  # Clip pixel values to range [0, 1]
plt.imshow(actual_image, interpolation='nearest')
ax.set_title('Actual label: %s' % class_labels[y_test[index].argmax()])
ax.axis('off')
plt.show()

# Display the predicted image and label
fig, ax = plt.subplots(figsize=(2, 2))
plt.imshow(X_test[index])
ax.set_title('Predicted label: %s' % class_labels[y_pred[index].argmax()])
ax.axis('off')
plt.show()

tf.keras.models.save_model(model,'my_model3.hdf5')

import numpy as np
import os
import cv2
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, classification_report
from keras.applications.vgg16 import VGG16, preprocess_input
from keras.layers import Dense, Flatten
from keras.models import Model
from keras.preprocessing.image import ImageDataGenerator

# Set the path to your dataset folder
dataset_path = "/content/sample_data/reduced cracks dataset/TRAIN DATA IMAGES"

# Set the input image size for VGG19
image_size = (224, 224)

# Load the VGG19 model
base_model = VGG16(weights='imagenet', include_top=False, input_shape=image_size + (3,))

# Freeze the pre-trained layers
for layer in base_model.layers:
    layer.trainable = False

# Add custom fully connected layers on top of VGG19
x = base_model.output
x = Flatten()(x)
x = Dense(512, activation='relu')(x)
x = Dense(256, activation='relu')(x)

# Calculate the number of classes based on the unique labels in the dataset
num_classes = len(os.listdir(dataset_path))

predictions = Dense(num_classes, activation='softmax')(x)

# Create the final model
model = Model(inputs=base_model.input, outputs=predictions)

# Compile the model
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Load and preprocess the images
images = []
labels = []
label_mapping = {}  # To map label indices to their respective labels
for i, label in enumerate(os.listdir(dataset_path)):
    label_mapping[i] = label
    label_path = os.path.join(dataset_path, label)
    for image_file in os.listdir(label_path):
        image_path = os.path.join(label_path, image_file)
        image = cv2.imread(image_path)
        image = cv2.resize(image, image_size)
        image = preprocess_input(image)
        images.append(image)
        labels.append(i)  # Use label index instead of one-hot encoding

# Convert images and labels to numpy arrays
images = np.array(images)
labels = np.array(labels)

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(images, labels, test_size=0.2, random_state=42)

# Perform data augmentation on the training set
train_datagen = ImageDataGenerator(rotation_range=20, width_shift_range=0.1, height_shift_range=0.1,
                                   shear_range=0.1, zoom_range=0.2, horizontal_flip=True, fill_mode='nearest')
train_datagen.fit(X_train)

# Convert labels to one-hot encoding
y_train_onehot = np.eye(num_classes, dtype=np.int32)[y_train]
y_test_onehot = np.eye(num_classes, dtype=np.int32)[y_test]

# Train the model
#history = model.fit(train_datagen.flow(X_train, y_train_onehot, batch_size=32), epochs=10)

# Train the model
history = model.fit(train_datagen.flow(X_train, y_train_onehot, batch_size=32), epochs=10,
                    validation_data=(X_test, y_test_onehot))

import matplotlib.pyplot as plt

#plt.figure(figsize=(8, 8))
#plt.subplot(1, 2, 1)
#plt.plot(range(EPOCHS), acc, label='Training Accuracy')
#plt.plot(range(EPOCHS), val_acc, label='Validation Accuracy')
#plt.legend(loc='lower right')
plt.title('VGG16 - Training and Validation Loss')
plt.xlabel("Epochs")
plt.ylabel("Accuracy")
plt.plot(history.history['loss'], label='train loss')
plt.plot(history.history['val_loss'], label='val loss')
plt.legend()
plt.grid()
plt.show()
plt.savefig('LossVal_loss')
# plt.subplot(1, 2, 2)
# plt.plot(range(EPOCHS), loss, label='Training Loss')
#plt.plot(range(EPOCHS), val_loss, label='Validation Loss')
# plt.legend(loc='upper right')
plt.title('VGG16 -Training and Validation Accuracy')
plt.xlabel("Epochs")
plt.ylabel("Loss")
# plt.show()
# accuracies
plt.plot(history.history['accuracy'], label='train_acc')
plt.plot(history.history['val_accuracy'], label='val_acc')
plt.legend()
plt.grid()
plt.show()
plt.savefig('AccVal_acc')

# Make predictions on the testing set
y_pred = model.predict(X_test)
y_pred = np.argmax(y_pred, axis=1)

# Compute the confusion matrix
confusion_mtx = confusion_matrix(y_test, y_pred)
print(confusion_mtx)

# Compute the true labels
true_labels = np.argmax(y_test_onehot, axis=1)

# Make predictions on the test dataset
y_pred = model.predict(X_test)
predicted_labels = np.argmax(y_pred, axis=1)

import numpy as np
import matplotlib.pyplot as plt
import itertools
from sklearn.metrics import confusion_matrix, classification_report

# Assuming you have a trained model called 'model' and the test dataset is stored in 'X_test' and 'y_test_onehot'
# model = ...
# X_test = ...
# y_test_onehot = ...

# Compute the true labels
true_labels = np.argmax(y_test_onehot, axis=1)
true_labels = [str(label) for label in true_labels]

# Make predictions on the test dataset
y_pred = model.predict(X_test)
predicted_labels = np.argmax(y_pred, axis=1)
predicted_labels = [str(label) for label in predicted_labels]

# Define the class labels
class_labels = ["Diagonal", "Horizontal", "Structural", "Vertical"]
#unique_classes = np.unique(np.concatenate((true_labels, predicted_labels)))
#class_labels = [str(cls) for cls in unique_classes]
#class_labels = np.unique(np.concatenate((true_labels, predicted_labels)))
#class_labels = list(X_train.class_indices.keys())


# Compute the confusion matrix
confusion_mtx = confusion_matrix(true_labels, predicted_labels)

# Function to plot the confusion matrix
def plot_confusion_matrix(cm, class_labels, normalize=False, title='VGG16 - Confusion Matrix', cmap=plt.cm.Oranges):
    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]

    plt.figure(figsize=(4,4))
    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title)
    plt.colorbar()
    tick_marks = np.arange(len(class_labels))
    plt.xticks(tick_marks, class_labels, rotation=90)
    plt.yticks(tick_marks, class_labels)

    fmt = '.2f' if normalize else 'd'
    thresh = cm.max() / 2.
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j, i, format(cm[i, j], fmt),
                 horizontalalignment="center",
                 color="white" if cm[i, j] > thresh else "black")

    plt.ylabel('True Label')
    plt.xlabel('Predicted Label')
    plt.tight_layout()

# Plot the confusion matrix
plot_confusion_matrix(confusion_mtx, class_labels, normalize=False)

# Show the plot
plt.show()

# Make predictions on the testing set
y_pred = model.predict(X_test)
y_pred = np.argmax(y_pred, axis=1)

# Compute the confusion matrix
confusion_mtx = confusion_matrix(y_test, y_pred)
print(confusion_mtx)

# Compute the true labels
true_labels = np.argmax(y_test_onehot, axis=1)

# Make predictions on the test dataset
y_pred = model.predict(X_test)
predicted_labels = np.argmax(y_pred, axis=1)

import numpy as np
import matplotlib.pyplot as plt
import itertools
from sklearn.metrics import confusion_matrix, classification_report

# Assuming you have a trained model called 'model' and the test dataset is stored in 'X_test' and 'y_test_onehot'
# model = ...
# X_test = ...
# y_test_onehot = ...

# Compute the true labels
true_labels = np.argmax(y_test_onehot, axis=1)
true_labels = [str(label) for label in true_labels]

# Make predictions on the test dataset
y_pred = model.predict(X_test)
predicted_labels = np.argmax(y_pred, axis=1)
predicted_labels = [str(label) for label in predicted_labels]

# Define the class labels
class_labels = ["Diagonal", "Horizontal", "Structural", "Vertical"]
#unique_classes = np.unique(np.concatenate((true_labels, predicted_labels)))
#class_labels = [str(cls) for cls in unique_classes]
#class_labels = np.unique(np.concatenate((true_labels, predicted_labels)))
#class_labels = list(X_train.class_indices.keys())


# Compute the confusion matrix
confusion_mtx = confusion_matrix(true_labels, predicted_labels)

# Function to plot the confusion matrix
def plot_confusion_matrix(cm, class_labels, normalize=False, title='VGG16 - Confusion Matrix', cmap=plt.cm.Oranges):
    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]

    plt.figure(figsize=(4, 4))
    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title)
    plt.colorbar()
    tick_marks = np.arange(len(class_labels))
    plt.xticks(tick_marks, class_labels, rotation=90)
    plt.yticks(tick_marks, class_labels)

    fmt = '.2f' if normalize else 'd'
    thresh = cm.max() / 2.
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j, i, format(cm[i, j], fmt),
                 horizontalalignment="center",
                 color="white" if cm[i, j] > thresh else "black")

    plt.ylabel('True Label')
    plt.xlabel('Predicted Label')
    plt.tight_layout()

# Plot the confusion matrix
plot_confusion_matrix(confusion_mtx, class_labels, normalize=False)

# Show the plot
plt.show()

# Print the classification report
classification_rep = classification_report(true_labels, predicted_labels, target_names=class_labels)
print(classification_rep)

# Select a random index from the test set
index = np.random.choice(len(X_test), 1)[0]
#index = np.random.choice(16, 1)[0]

# Display the actual image and label
fig, ax = plt.subplots(figsize=(2, 2))
actual_image = np.clip(X_test[index], 0, 1)  # Clip pixel values to range [0, 1]
plt.imshow(actual_image, interpolation='nearest')
ax.set_title('Actual label: %s' % class_labels[y_test[index].argmax()])
ax.axis('off')
plt.show()

# Display the predicted image and label
fig, ax = plt.subplots(figsize=(2, 2))
plt.imshow(X_test[index])
ax.set_title('Predicted label: %s' % class_labels[y_pred[index].argmax()])
ax.axis('off')
plt.show()

tf.keras.models.save_model(model,'my_model4.hdf5')

import os
from tensorflow.keras.models import load_model

# Define the directory where you want to save your models
save_dir = '/content/saved_models'

# Assuming you have models named as model1, model2, ..., model4
my_model1 = load_model("my_model1.hdf5")
my_model2 = load_model("my_model2.hdf5")
my_model3 = load_model("my_model3.hdf5")
my_model4 = load_model("my_model4.hdf5")
model_variables = [globals()[f"my_model{i}"] for i in range(1, 5)]  # Replace with your actual model variables
model_names = ["my_model1.hdf5", "my_model2.hdf5", "my_model3.hdf5", "my_model4.hdf5"]

# Create the save directory if it doesn't exist
os.makedirs(save_dir, exist_ok=True)

# Iterate over models and save them
for model, model_name in zip(model_variables, model_names):
    model.save(os.path.join(save_dir, model_name))

# Optionally, you can load the saved models back
loaded_models = [load_model(os.path.join(save_dir, model_name)) for model_name in model_names]

!pip install streamlit

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import streamlit as st
# import tensorflow as tf
# 
# st.set_option('deprecation.showfileUploaderEncoding',False)
# @st.cache(allow_output_mutation=True)
# def load_model():
#   model=tf.keras.models.load_model('/content/saved_models/my_model1.hdf5')
#   model=tf.keras.models.load_model('/content/saved_models/my_model2.hdf5')
#   model=tf.keras.models.load_model('/content/saved_models/my_model3.hdf5')
#   model=tf.keras.models.load_model('/content/saved_models/my_model4.hdf5')
#   return model
#  model=load_model()
#  st.write("""
#          #Crack Detection
#          """
#          )
# 
# file = st.file_uploader("please upload an crack image", type=["jpg", "png"])
# import cv2
# import PIL import Image, ImageOps
# import numpy as np
# def import_and_predict(image_data, model):
# 
# size = (180,180)
# image = ImageOps.fit(image_data, size, Image.ANTIALIAS)
# img = np.asarray(image)
# img_reshape = img[np.newaxis,...]
# prediction = model.predict(img_reshape)
# 
# return prediction
# 
# if file is None:
# st.text("please upload an image file")
# else:
# image = Image.open(file)
# st.image(image,use_column_width=True)
# predictions = import_and_predict(image, model)
# class names['diagonal','horizontal','structural','vertical']
# string="This image most likely is: "+class_names[np.argmax(predictions)]
# st.success(string)
# 
#

